---
title: "Hierarchical ordination"
output: html_document
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
```

# An Example of Hierarchical Ordination

This document describes fitting a hierarchical ordination to data, including code and the details that are needed.

The data is included in the *mvabund* *R*-package, but was originally collected by [Gibb *et al.*](https://link.springer.com/article/10.1007/s00442-014-3101-9). It is includes abundance observations of 41 ant species at 30 sites in Australia, along with 7 environmental variables and 5 traits. The aim to is determine how the environmental variables and traits affect composition in the ecological community, including whether they interact. The methodological question is how does hierarchical ordination help.

The abundances, traits and environment are each stored in a different matrix. First we load the data and set some constants:

```{r data, cache=FALSE, message=FALSE}
library(nimble)
library(nimbleHMC)
library(mvabund)
library(coda)

data("antTraits")
Y <- antTraits$abund # abundance data of sites by species
X <- scale(antTraits$env) # environmental variables of sites by predictors
qrX <- qr(X)
X <- X[,qrX$pivot[1:qrX$rank]]
TR <- scale(model.matrix(~0+.,antTraits$traits)) # species traits
qrTR <- qr(TR)
TR <- TR[,qrTR$pivot[1:qrTR$rank]]
NSites <- nrow(Y) # number of sites
NSpecies <- ncol(Y) # number of species
NTraits <- ncol(TR) # number of traits
NEnv <- ncol(X) # number of environmental predictors

# create data lists for Nimble
dat <- list(Y = Y, X = X, TR = TR)
consts <- list(NSites = NSites, NEnv = NEnv, NTraits = NTraits, NSpecies = NSpecies)
rm(NEnv, NTraits, NSpecies, NSites)
```

## The Model

The data are counts of each species so we assume they follow a Poisson distribution with a log link function, as we would do in a standard generalised linear model. We assume that each species has a different mean abundance (i.e. for each species $j$ we have a different intercept $\beta_{0j}$), and model the rest of the variation with a hierarchical ordination. This gives the following mean model on the link scale (with linear predictor $\eta_{ij}$):

$$
\eta_{ij} = \beta_{0j} + \boldsymbol{z}_i^\top \boldsymbol{\Sigma} \boldsymbol{\gamma}_j.
$$

As with any ordination, $\boldsymbol{z}$ and $\boldsymbol{\gamma}_j$ are the site scores and species loadings, and the columns of $\boldsymbol{Z}$ are the latent variables (which holds the site scores as the rows). Here we assume that they each have a variance of one, so that $\boldsymbol{\Sigma}$ holds the variation of the latent variables: it will typically be a diagonal matrix, and if any of the terms on the diagonal are close to zero, this suggests that latent variable has (almost) no effect. It thus provides a straightforward summary of the relative importance of that latent variable, and is similar to the square root of eigenvalues (singular values) in an eigenanalysis.

If we stopped here, this would be a standard Generalized Linear Latent Variable Model. But, here we want to model both $\boldsymbol{z}_i$ and $\boldsymbol{\gamma}_j$, i.e. go from modelling model groups of species responding in similar ways to sites to modelling how the traits of the species affect their responses to the environment.

As a simplification, we can think about this as simply a regression of $\boldsymbol{z}_i$ (the site effects) against the environmental variables and $\boldsymbol{\gamma}_j$ against the traits. In reality, it is more complicated, because $\boldsymbol{z}_i$ and $\boldsymbol{\gamma}_j$ are estimated in the model, so their uncertainty needs to be propagated.

<details><summary>Some more mathematical details are hidden away here, for those who are interested.</summary>
We denote the abundance at site $i = 1 \dots n$ for species $j = 1 \dots p$ as $Y_{ij}$, or as a matrix as $\boldsymbol{Y}$. The environmental variables are $x_{ik}$ for the $k = 1 \ldots K$ predictors (or $\boldsymbol{X}$ as a matrix), and $t = 1\ldots T$ traits as $\boldsymbol{TR}$. We then assume

$$Y_{ij} \sim \text{Pois}(\lambda_{ij}),$$

with

$$\text{log}(\lambda_{ij}) = \eta_{ij}.$$

Consequently, $\eta_{ij}$ is the linear predictor, which we further model with the hierarchical ordination:

$$
\eta_{ij} = \beta_{0j} + \boldsymbol{z}_i^\top \boldsymbol{\Sigma} \boldsymbol{\gamma}_j.
$$

We then model $\boldsymbol{z}_i$ (as in [van der Veen *et al.* (2023)](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.14035)) and $\boldsymbol{\gamma}_j$ hierarchically:

$$
\boldsymbol{z}_i = \boldsymbol{B}^\top\boldsymbol{x}_i + \boldsymbol{\epsilon}_i
$$
and 

$$
\boldsymbol{\gamma}_j = \boldsymbol{\omega}^\top\boldsymbol{TR}_{j} + \boldsymbol{\varepsilon}_j
$$
where:

- $x_{ik}$ is the $k^{th}$ predictor (i.e. environmental effect) at site $i$ 
- $\boldsymbol{B}$ with entry $b_{kq} \sim \mathcal{N}(0,1)$ is the effect of the $k^{th}$ predictor on the site score for the $q^{th} = 1\ldots d$ latent variable
- $\boldsymbol{\epsilon}_i$ with entry $\epsilon_{iq} \sim \mathcal{N}(0, \sigma^2_q)$ is a vector of residuals for the unexplained part of the site score
- $TR_{jt}$ is the $t^{th}$ predictor (i.e. trait) for species $j$
- $\boldsymbol{\omega}_t$ with entry $\omega_{tq}$ is the effect of the $t^{th}$ trait on the species loading for the $q^{th}$ latent variable
- $\boldsymbol{\varepsilon}_j$ with entry $\varepsilon_{jq}  \sim \mathcal{N}(0, \delta^2_q)$ is a vector of residuals for the unexplained part of the species loading

Note that the predictors are all standardized to zero mean and unit variance. We additionally place exponential priors with rate parameter one on all the scale parameters.
</details>


# Implementation 

We fit the model with the *Nimble* *R*-package. We start with a single dimension for simplicity, so that we can show the steps needed.

### One dimensional ordination

To fit the model we need to 1) code up the likelihood, 2) specify the Markov Chain Monte carlo samplers and initial values, 3) run it. We parallelise running the MCMC so that each chain is run on a different processor. This means we need a function to run one chain, and then another to run all of the chains together.

<details><summary>The Nimble code for the model and likelihood is here</summary>

```{r model_code, message=FALSE, warning=FALSE}
OneLV.nim <- nimbleCode({
  for (i in 1:NSites) {
    for (j in 1:NSpecies) {
# These three lines specify the model:   
      Y[i, j] ~ dpois(lambda[i, j]) # Likelihood: Y follows a Poisson distribution
      log(lambda[i, j]) <- eta[i, j] # Specify the log link function
      eta[i,j] <- beta0[j] + gamma[j]*sd.LV*z[i] # linear predictor: species + LV
    }
# Site scores: regression against environmental effects
#    the Bs are the coefficients of the environmental effects
      epsilonSTAR[i] ~ dnorm(0, sd = sd.Site) # Residual site effect
      XB[i] <- inprod(X[i, 1:NEnv],BSTAR[1:NEnv])
      zSTAR[i] <- XB[i] + epsilonSTAR[i]
  }
  
  for(j in 1:NSpecies) {
# Species effects: regression against trait effects
#    The Os are the trait effects.
      varepsilonSTAR[j] ~ dnorm(0, sd = sd.Species) # Residual
      omegaTR[j] <- inprod(TR[j, 1:NTraits],OSTAR[1:NTraits])
      gammaSTAR[j] <- omegaTR[j] + varepsilonSTAR[j]
      beta0[j] ~ dnorm(0, sd = 1) # Species means
  }
  
# Here we standardise z and gamma, so the both have a variance of 1.
#   The standard deviation of the latent variable is sd.LV.
    StdDev.z <- sd(zSTAR[1:NSites])
    StdDev.gamma <- sd(gammaSTAR[1:NSpecies])
    z[1:NSites] <- zSTAR[1:NSites]/StdDev.z
    gamma[1:NSpecies] <- gammaSTAR[1:NSpecies]/StdDev.gamma
    
# Scale other parameters
    epsilon[1:NSites] <- epsilonSTAR[1:NSites]/StdDev.z
    B[1:NEnv] <-  BSTAR[1:NEnv]/StdDev.z
    varepsilon[1:NSpecies] <- varepsilonSTAR[1:NSpecies]/StdDev.gamma
    O[1:NTraits] <-  OSTAR[1:NTraits]/StdDev.gamma

    # priors for scales
    sd.Site ~ dexp(1)
    sd.Species ~ dexp(1)
    sd.LV ~ dexp(1)

    for(k in 1:NEnv){
      BSTAR[k] ~ dnorm(0, sd = 1)  
    }
    for(tr in 1:NTraits){
      OSTAR[tr] ~ dnorm(0, sd = 1)
    }
})
sampler_glm_pois <- nimbleFunction(
  
  contains = sampler_BASE,
  name = 'sampler_glm_pois',
  setup = function(model, mvSaved, target, control) {
    # some defensive programming
    # assigning the sampler to only some components of a multivariate node will
    # result in the wrong posterior
    targetCollected <- model$expandNodeNames(target) # not sure if multivariate nodes can ever come as series of univariates
    n.target.collected <- length(targetCollected)
    targetAsScalar <- model$expandNodeNames(target, returnScalarComponents = TRUE)
    n.param.nodes <- length(targetAsScalar) # number of target parameters
    if(n.target.collected != n.param.nodes){
      stop("This sampler should be applied to all components of a multivariate node simultaneously.")
    }
    priorDist <- unlist(sapply(targetCollected,model$getDistribution))
    # find hyper parameter index
    priorParID <- integer(n.target.collected)
    for(i in 1:n.target.collected){
      priorParID[i] <- which(names(getDistributionInfo(model$getDistribution(targetCollected[i]))$paramIDs)%in%c("var","cov"))
    }
    # this sampler only works for normally distributed nodes
    if(!all(priorDist%in%c("dnorm","dmnorm"))){
      stop("This sampler requires target parameters to have normally distributed priors.")
    }
    calcNodes <- model$getDependencies(target)
    # check if initial values were assigned, if not simulate from prior
    if(is.na(model$calculate())){
    model$simulate()     
    model$calculate(calcNodes)
    }
    # get some information about the model
    response.nodes.names <- names(which(model$isStoch(names(model$origData)))) # names of response nodes
    if(!all(model$getDistribution(response.nodes.names)=="dpois"))stop("This sampler is designed only for Poisson responses.")
    response.name  = unique(gsub("\\[.*","",response.nodes.names))
    response.nodes <- values(model, response.nodes.names) # response data
    n.response.nodes <- length(response.nodes) # number of observations
    n.zero.response.nodes = sum(response.nodes==0) # number of zero observations
    ## get distribution parameter names
    dist.par.names = model$getParents(response.nodes.names,determOnly = F, immediateOnly = T)
    distpar.name  = unique(gsub("\\[.*","",dist.par.names))
    lp.nodes.names = model$getParents(dist.par.names,immediateOnly = T, determOnly = T,includeData = F)
    lp.name  = unique(gsub("\\[.*","",lp.nodes.names))
    # get corresponding "data" nodes to target
    # we don't have information on the data, so we do this by iterating through the linear predictor
    # parameter by parameter
    X <- matrix(0, nrow = n.response.nodes, ncol = n.param.nodes)
    inits <- values(model, targetAsScalar) # temporarily store parameters
    # get any constant parts in the lp
    lp.temp.setup <- values(model, lp.name)
    values(model, targetAsScalar) <<- 2*inits
    model$calculate(lp.name)
    lpCon <- 2*lp.temp.setup-values(model, lp.name)
    values(model, targetAsScalar) <<- inits
    # calculate "X" from the linear predictor
    for(i in 1:n.param.nodes){
    par <- inits[i]
    values(model, targetAsScalar[i]) <<- 0
    model$calculate(lp.name)
    temp.lp1 <- values(model, lp.name)
    values(model, targetAsScalar[i]) <<- inits[i]
    X[,i] <- (lp.temp.setup-temp.lp1)/par
    }
    rm(temp.lp1,lp.temp.setup)
    # Set-up the prior parameters
    priorMean = matrix(0,ncol=1,nrow=n.param.nodes)
    priorCov = matrix(0, ncol = n.param.nodes, nrow = n.param.nodes)
    # need to turn this into a function that accepts a bunch of indices so i can update hyperpars mid-run
    # if all node entries are in the target, sample as is. otherwise, we need the conditional means and covariance.
    # so, check if "target" has subset entries of a multivariate node
    # means
      k.setup <- 1
      for(i in 1:n.target.collected){
      means.setup = model$getParam(targetCollected[i], 'mean')
      priorMean[k.setup:(k.setup+length(means.setup)-1),1] <- means.setup
      k.setup <- k.setup+length(means.setup)
      }
      rm(means.setup, k.setup)
    # covariances
      k.setup <- 1
      for(i in 1:n.target.collected){
      covs.setup = as.matrix(model$getParam(targetCollected[i], priorParID[i]))
      dims.setup = dim(covs.setup)[1]
      priorCov[k.setup:(k.setup+dims.setup-1),k.setup:(k.setup+dims.setup-1)] <- covs.setup
      k.setup <- k.setup+dims.setup
      }
      rm(covs.setup, k.setup, dims.setup)
    # check if there are hyperparameters in the model that might need updating mid-run
    hasHyperPar <- length(model$getParents(target))>0
    # check if there are other parameters in the model than that we are sampling
    # but filter it by potential parent nodes, i.e., hyper parameters
    hasOtherLPpar <- (length(model$expandNodeNames(model$getNodeNames(stochOnly = T,includeData = F)))-length(model$getParents(model$expandNodeNames(model$getNodeNames(stochOnly = T,includeData = F)))))>length(targetAsScalar)
    # mixture components
    # v: variances (s), m: means (m), w: weights
    mcomp <- pogit:::mixcomp_poisson()
    mcompm <- mcomp$m
    mcompv <- mcomp$v
    mcompw <- mcomp$w
    # start by getting mixture components, this is "get_mixcomp" in pogit
    # m: means for mixture components, v: scales, w: weights
    if(sum(response.nodes<3e4)==n.response.nodes){
      my = mcomp$m[response.nodes[response.nodes>0],]
      vy = mcomp$v[response.nodes[response.nodes>0],]
      wy = mcomp$w[response.nodes[response.nodes>0],]
    }else{
      wy = vy = my = matrix(0, nrow = n.response.nodes - n.zero.response.nodes, ncol = 10)
      my[response.nodes[response.nodes>0] <= 3e4,1:dim(mcomp$m)[2]] = mcomp$m[response.nodes[(response.nodes>0) & (response.nodes <= 3e4)],]
      vy[response.nodes[response.nodes>0] <= 3e4,1:dim(mcomp$v)[2]] = mcomp$v[response.nodes[(response.nodes>0) & (response.nodes <= 3e4)],]
      wy[response.nodes[response.nodes>0] <= 3e4,1:dim(mcomp$w)[2]] = mcomp$w[response.nodes[(response.nodes>0) & (response.nodes <= 3e4)],]
      # this comes from "compute_mixture.R"
      for(i in response.nodes[response.nodes>3e4]){
        my[response.nodes[response.nodes>0]==i,1] <- -digamma(i)
        vy[response.nodes[response.nodes>0]==i,1] <- trigamma(i)
        wy[response.nodes[response.nodes>0]==i,1] <- 1 
      }
    }
    vy[is.na(vy)] <- 0
    my[is.na(my)] <- 0
    wy[is.na(wy)] <- 0
    # for use in step 3
    # R1
    c1 <- t(matrix(log(mcompw[1, ]) - 0.5*log(mcompv[1, ]), nrow = 10, ncol = n.response.nodes)) # not sure what this is yet
    # R2
    vy2 <- vy
    vy2[vy2==0]<-1
    lwy <- log(wy)
    lwy[is.infinite(lwy)] <- 0
    lvy <- log(vy)
    lvy[is.infinite(lvy)] <- 0
    c2 <- (lwy - 0.5*lvy) 
    kill <- matrix(vy > 0, n.response.nodes - n.zero.response.nodes, 10)
  },
  run = function() {
    
    if(hasOtherLPpar){
    # -1. Update "lpCon", i.e., "fixed" portion of LP
    pars <- values(model, target) # temporarily store parameters
    lp.temp <- values(model, lp.name)
    values(model, targetAsScalar) <<- 2*pars
    model$calculate(lp.name)
    lpCon <<- 2*lp.temp-values(model, lp.name)
    values(model, targetAsScalar) <<- pars # restore parameters
    model$calculate(lp.name)
    }
    
    if(hasHyperPar){
    # 0. retrieve new hyperparameters
      k <- 1L
      for(i in 1:n.target.collected){
      dims <- length(values(model, targetCollected[i]))
      idx <- k+dims-1L
      priorMean[k:idx,1] <<- nimMatrix(model$getParam(targetCollected[i], 'mean'),ncol = 1)
      temp = model$getParam(targetCollected[i], priorParID[i])
      priorCov[k:idx,k:idx] <<- nimMatrix(temp, ncol = dims, nrow = dims) # covariances
      k <- k+dims
      }
    }

    ## get LP and lambda
    lp = values(model, lp.nodes.names)
    lambda = exp(lp)# sampler is only valid for models with the log-link # values(model, dist.par.names)
    #1. update tau's, step 1 in Fruhwirth-Schnatter et al. 2009
    # this part is covered in dataug_pois_iams.R->iams1_poisson
    # tau are latent variables that represent the inter-arrival times of a Poisson process
    # get_mixcomp_poisson in dataug_pois_iams.R just gets components that we need for the other functions
    
    tau <- update_tau(response.nodes, n.response.nodes, n.zero.response.nodes, lambda)
    t1 <- tau[,1]
    t2 <- tau[response.nodes>0,2]
    # 2. update R's, step 2 in Fruhwirth-Schnatter et al. 2009
    # this part is covered in dataug_pois_iams.R->iams2_poisson
    # Rmix are normal mixture approximations to Poissons
    
    R1 <- update_R1(n.response.nodes, lp, t1, mcompm, mcompv, c1)
    R2 <- update_R2(response.nodes, n.response.nodes, n.zero.response.nodes, lp, t2, vy, my, vy2, c2, kill)
    
    # 3. simulate parameters given R, tau
    # this part is covered in "select_poisson.R"
    # calculate posterior moments
    proposal <- simulate_post(X, n.response.nodes, n.zero.response.nodes, n.param.nodes, response.nodes, R1, R2, tau, mcompv, mcompm, my, vy, priorMean, priorCov, lpCon)
    # store it
    values(model, targetAsScalar) <<-  proposal
    # update nodes based on proposal
    model$calculate()
    # keep the model and mvSaved objects consistent
    copy(from = model, to = mvSaved, row = 1, 
         nodes = target, logProb = TRUE)
  },
  methods = list(simulate_post = function(X = double(2), n.response.nodes = integer(), n.zero.response.nodes = integer(), n.param.nodes = integer(), response.nodes = integer(1), R1 = integer(1), R2 = integer(1), tau = double(2), mcompv = double(2), mcompm = double(2), my = double(2), vy = double(2), priorMean = double(2), priorCov = double(2), lpCon = double(1)){
    m1 <- mcompm[1,R1]
    m2 <- numeric(n.response.nodes-n.zero.response.nodes)
    for(i in 1:10){
      m2[R2==i] <- my[R2==i, i]
    }
    # second: mixture component variances
    v1 <- mcompv[1,R1]
    v2 <- numeric(n.response.nodes-n.zero.response.nodes)
    for(i in 1:10){
      v2[R2==i] <- vy[R2==i, i]
    }
    invSigS = numeric(n.response.nodes)
    ys1 = (-log(tau[,1])-m1)/sqrt(v1) # this is also "AuxMixPoisson:value()" in JAGS
    ys2 = (-log(tau[response.nodes>0,2])-m2)/sqrt(v2)
    invSigS = 1/v1
    invSigS[response.nodes>0] = invSigS[response.nodes>0]+1/v2
    # prior parameters
    priorPrec <- inverse(priorCov)
    # posterior parameters
    postPrec <- priorPrec + t(X)%*%diag(invSigS)%*%X
    postMean <- solve(postPrec, priorPrec%*%priorMean+ (t(X)%*%diag(1/sqrt(v1))%*%ys1+t(X[response.nodes>0,])%*%diag(1/sqrt(v2))%*%ys2)) # see eq (8) Fruhwirth-Schnatter and Wagner 2006 or L109-111 from pogit::select_poisson.Rs
    # a sample from the posterior
    proposal <- rmnorm_chol(1, mean = c(postMean), cholesky = chol(postPrec), prec_param = TRUE)
    return(proposal)
    returnType(double(1))
},
                 update_tau = function(response.nodes = integer(1), n.response.nodes = integer(), n.zero.response.nodes = integer(), lambda = double(1)){
                        taunew = matrix(0, nrow = n.response.nodes, ncol = 2)
                        taunew[,1] <- rexp(n.response.nodes, lambda)
                        tau2 <- rbeta(n.response.nodes - n.zero.response.nodes, response.nodes[response.nodes>0], 1)
                        taunew[response.nodes > 0,  1] = 1-tau2 + taunew[response.nodes > 0,1]
                        taunew[response.nodes > 0,  2]  = tau2
                        taunew[response.nodes == 0, 1] = 1 +  taunew[response.nodes == 0, 1]
                        return(taunew)
                        returnType(double(2))
                      },
                update_R1 = function(n.response.nodes = integer(), lp = double(1), t1 = double(1), mcompm = double(2), mcompv = double(2), c1 = double(2))
                      {
                        rgm <- matrix(0, ncol = 10, nrow = n.response.nodes)
                        for(i in 1:10){
                          rgm[,i] <- c1[,i] - 0.5*(-log(t1)-lp - mcompm[1,i])^2/mcompv[1,i]
                          rgm[rgm[,i]==0,i] <- -Inf
                        }
                        mx <- numeric(n.response.nodes)
                        e1 <- matrix(0, ncol = 10, nrow = n.response.nodes)
                        for(i in 1:n.response.nodes){
                          mx[i] <- max(rgm[i,])
                          if(mx[i]==-Inf)mx[i]=0
                          e1[i,] <- exp(rgm[i,]-mx[i])
                        }
                        # row sum
                        rgmod <- e1%*%matrix(1,nrow=10,ncol=1)
                        
                        e1.new = matrix(0, ncol = 10, nrow = n.response.nodes)
                        for(i in 1:10){
                          e1.new[,i] <- e1[,i]/rgmod # for safety might want to check for 0/0 i.e., nan
                        }
                        tri.mat = matrix(1, 10, 10)
                        for (j in 1:9) {
                          for (i in (j+1):10) {
                            tri.mat[i,j] = 0
                          }
                        }
                        Fn    <- e1.new%*%tri.mat
                        
                        # determination of random indicators R1
                        # inverse transform sampling Poisson-Binomial
                        u <- runif(n.response.nodes, 0, 1)
                        R1.temp <- nimMatrix(0, ncol = 10, nrow = n.response.nodes, type = "integer")
                        for(i in 1:10){
                          R1.temp[,i] <- u<Fn[,i]
                        }
                        # declare R1new explicitly as integer
                        R1new <- integer(n.response.nodes)
                        R1new[1:n.response.nodes] <- 11L-c(R1.temp%*%nimMatrix(1,nrow=10,ncol=1, type = "integer"))
                        
                        return(R1new)
                        returnType(integer(1))
                      },
                update_R2 = function(response.nodes = integer(1), n.response.nodes = integer(), n.zero.response.nodes = integer(), lp = double(1), t2 = double(1), vy = double(2), my = double(2), vy2 = double(2), c2 = double(2), kill = logical(2)){
                        rgmx <- matrix(0, ncol = 10, nrow = n.response.nodes - n.zero.response.nodes)
                        for(i in 1:10){ #ncol r gm
                          rgmx[,i]   <- c2[,i] - 0.5*((-log(t2)-lp[response.nodes>0]- my[,i])*kill[,i])^2/vy2[,i]
                          rgmx[rgmx[,i]==0,i] <- -Inf
                        }
                        e2 <- matrix(0,nrow = n.response.nodes-n.zero.response.nodes, ncol = 10)
                        mx2 = numeric(n.response.nodes-n.zero.response.nodes)
                        for(i in 1:(n.response.nodes-n.zero.response.nodes)){
                          mx2[i] = max(rgmx[i,])
                          if(mx2[i]==-Inf)mx2[i]=0
                          e2[i,] <- exp(rgmx[i,]-mx2[i])
                        }
                        # row sum
                        rgmodx <- e2%*%matrix(1,nrow=10,ncol=1)

                        e2.new = matrix(0,nrow=n.response.nodes-n.zero.response.nodes, ncol = 10)
                        for(i in 1:10){
                          e2.new[,i] <- e2[,i]/rgmodx # for safety might want to check for 0/0 i.e., nan
                        }
                        
                        tri.mat = matrix(1, 10, 10)
                        for (j in 1:9) {
                          for (i in (j+1):10) {
                            tri.mat[i,j] = 0
                          }
                        }
                        
                        Fx     <- e2.new%*%tri.mat
                        
                        # determination of random indicators R2
                        # inverse transform sampling Poisson-Binomial
                        ux <- runif(n.response.nodes - n.zero.response.nodes, 0, 1)
                        R2.temp <- nimMatrix(0, ncol = 10, nrow = n.response.nodes - n.zero.response.nodes, type = "integer")
                        for(i in 1:10){
                          R2.temp[,i] <- ux<Fx[,i]
                        }
                        # declare R2new explicitly as integer
                        R2new <- integer(n.response.nodes - n.zero.response.nodes)
                        R2new[1:(n.response.nodes-n.zero.response.nodes)] <- 11L-R2.temp%*%nimMatrix(1,nrow=10,ncol=1, type = "integer")
                        
                        return(R2new)
                        returnType(integer(1))
                      },
                 reset = function () {}
                    )
)
```
</details>

<details><summary>Code to generate initial values is here 
</summary>

```{r init}
inits <- function(consts){
  B = rnorm(consts$NEnv)
  O = rnorm(consts$NTraits)
  varepsilon = rnorm(consts$NSpecies)
  epsilon = rnorm(consts$NSites)
  list(
    BSTAR = B,
    OSTAR = O,
    epsilonSTAR = epsilon,
    varepsilonSTAR = varepsilon,
    sd.Site = rexp(1),
    sd.Species = rexp(1),
    beta0 = rnorm(consts$NSpecies),
    sd.LV = rexp(1)
  )
}
```

</details>

<details><summary>Functions for MCMC are here.</summary>

```{r mcmc_code}
# Function to run one chain: it can be done with HMC or other MCMC algorithms.
# "block" can be used to specify blocking structures for the slice sampler
# "slice" can be used to specify parameters on which to apply univariate Metropolis-Hastings
# parameters that are not in "block" or "slice" are HMCed
RunOneChain <- function(seed, dat, code, inits, consts, ToMonitor=NULL, 
                        Nburn=5e3, NIter=5.5e4, Nthin=10, block = NULL, slice = NULL, ...) {
  require(nimble)
  require(nimbleHMC)
  AllSamplers <- HMCsamplers <- c('epsilonSTAR', 'varepsilonSTAR', 'beta0', 'OSTAR', 'BSTAR',
                  'sd.Site', 'sd.Species', 'sd.LV')
  
  if(!is.null(block)){
  HMCsamplers <- HMCsamplers[!HMCsamplers%in%unique(gsub("\\s*\\[[^\\)]+\\]","",c(unlist(block),unlist(slice))))]
  }
  if(is.null(ToMonitor)) {
    ToMonitor <- c("beta0", "sd.Species", "sd.Site", "sd.LV", "B", "O", 
                   "epsilon", "varepsilon", "gamma", "z")
  }
  mod <- nimbleModel(code = code, name = "HO", constants = consts, 
                     inits = inits(consts), data = dat, buildDerivs = TRUE)
  model <- compileNimble(mod)
  
  # Do HMC
    conf <- configureHMC(model, nodes = HMCsamplers, monitors = ToMonitor, print = FALSE, 
                         control=list(nwarmup=Nburn))
    if(!is.null(block)) { 
    if(is.list(block)){
    # Use a slice everything that not being HMCed
      lapply(block, conf$addSampler, type = "sampler_glm_pois")
    }else{
      # Use a slice everything that not being HMCed
      sapply(block, conf$addSampler, type = "sampler_glm_pois")
    }
    }
    
    if(!is.null(slice)) { 
    if(is.list(slice)){
    # Use a slice everything that not being HMCed
      lapply(slice, conf$addSampler, type = "sampler_glm_pois")
    }else{
      # Use a slice everything that not being HMCed
      sapply(slice, conf$addSampler, type = "sampler_glm_pois")
    }
    }
    
  mcmc <- buildMCMC(conf)
  cmcmc <- compileNimble(mcmc, project = model)
  res <- runMCMC(cmcmc,  niter=NIter, nburnin = Nburn, thin=Nthin, 
                 nchains = 1, samplesAsCodaMCMC = TRUE, ...)
  return(res)
}

# Function to run parallel chains
ParaNimble <- function(NChains, ...) {
  opts <- list(...)
  if(!is.null(opts$seeds) && (length(opts$seeds) == NChains)){
    seeds <- opts$seeds
    opts <- opts[-which(names(opts)=="seeds")]
  }else{
    seeds <- 1:NChains
  }
  require(parallel)
  nimble_cluster <- makeCluster(NChains)
  clusterExport(cl = nimble_cluster, "sampler_glm_pois")
  samples <- parLapply(cl = nimble_cluster, X = seeds, ...)
  stopCluster(nimble_cluster)

  # Name the chains in the list
  chains <- setNames(samples,paste0("chain", 1:length(samples)))
  chains
}

# Function to create list of names for parameters to use a block slice sampler for
MakeBlockList <- function(consts, LVwise = TRUE){
# builds list for slice AF sampling
# with LVwise = TRUE we are LV-wise blocking B and epsilon, and O and varepsilon
# otherwise, LVs are jointly blocked
if(LVwise & consts$nLVs>1){
blockList <- c(sapply(1:consts$nLVs,
       function(x,consts){
         c(paste0("BSTAR[", 1:consts$NEnv,", ",x, "]"),
           paste0("epsilonSTAR[", 1:consts$NSites,", ",x, "]"))
         }
       ,
         consts = consts,simplify=F),
       sapply(1:consts$nLVs,
       function(x,consts){
         c(paste0("OSTAR[", 1:consts$NTraits,", ",x, "]"),
           paste0("varepsilonSTAR[", 1:consts$NSpecies,", ",x, "]"))
         }
       ,
         consts = consts,simplify=F)
       )
if(consts$nLVs>1){
# remove entries of B,O-STAR that are fixed
for(q in 2:consts$nLVs){
  blockList[[q]] <- blockList[[q]][-(1:(q-1))]
  blockList[-c(1:consts$nLVs)][[q]] <- blockList[-c(1:consts$nLVs)][[q]][-(1:(q-1))]
}
}
}else{
  blockList = list(c("BSTAR","epsilonSTAR"),c("OSTAR","varepsilonSTAR"))
}
blockList
}

```

</details>

Latent variable models are notorious for being unidentifiable, you can get the same mean abundances from different combinations of the parameters. We have to make some adjustments to the model to account for this: some of this is done in the model fitting, but for others it is easier to do it after we obtain the posterior samples.

<details><summary>The details of what we do to make the HO identifiable are here</summary>

- First, we standardise $\boldsymbol{z}_i$ and $\boldsymbol{\gamma}_j$ to unit variance per latent variable to prevent scale invariance
- At this point the model is still invariant to sign switching: because $z_{i}\gamma_{j} = (-z_{i})(-\gamma_{j})$ in the MCMC algorithm can (and does) switch signs mid run. We could solve it by placing a truncated normal prior on the main diagonal entries of $\boldsymbol{B}$ or $\boldsymbol{\omega}$, but that results in bimodal posterior distributions for other coefficients. Instead we impose sign constraints by post-process the chains. We choose one parameter, for example one $z_{i}$ or $\gamma_{j}$, to be positive, and switch all of the other parameters based on that one. See below for the details of how we fix the signs.

The functions to swap the signs are included below. We want to make one parameter positive, so ideally we want to do this to a parameter that has both modes away from zero. We can identify this in an **ad hoc** way: for each chain for every parameter we calculate the proportion of iterations where the sign is positive, and then for every parameter we calculate the variance in that proportion. If a parameter is centered around 0 the mean proportion will be about 0.5, and will not vary much, whereas if it is some way from 0 the mean will be close to 0 or 1, and the variance will be high. ALternativley, we coiuld look at the largest $|p - 0.5|$, or we can vissually identify such parameters from the posterior samples. There may be even better alternatives.

```{r ProcessFunction}

# Utility Function to get logical indicators for if names contains a string in v
GetInds <- function(v, names) {
  if(length(v)==1) {
    res <- grep(v, names)
  } else {
    res <- c(unlist(sapply(v, grep, x=names)))
  }
  res
}

# Function to swap signs of all variables varinds to have same sign as vartosign.
ReSignChain <- function(chain, varinds, vartosign) {
  #    MeanSign <- sign(mean(chain[  ,s])) # Might need this
  res <- t(apply(chain, 1, function(x, vs, vi) {
    Names <- names(x)[vi]
    if(any(grepl(",", Names))) {
      lvind <- gsub(".*, ", "", Names)
    } else {
      lvind <- seq_along(vs)
    }
    x[vi] <- x[vi]*sign(x[vs[lvind]])
    x
  }, vs=vartosign, vi=varinds))
  as.mcmc(res)
}



# Function to post-process chains to swap signs.
postProcess <- function(Chains, VarsToProcess, VarsToSwapBy = NULL, VarToSign=NULL, print=FALSE, rule = 2) {
  if(is.null(VarToSign)) VarToSign <- VarsToProcess
  SignInd <- GetInds(VarToSign, colnames(Chains[[1]]))
  # Get indicators for all variables to have their signs changed
  ProcessInds <- GetInds(VarsToProcess, names = colnames(Chains[[1]]))
  
  # Check if > 1 LV
  SeveralLVs <- any(grepl(",", colnames(Chains[[1]])[SignInd]))
  LV <- gsub(".*, ", "", colnames(Chains[[1]])[SignInd])
  
  if(rule==1){
  # Calculate variance of mean of indicator of sign: 
  # hopefully largest is variable with most sign swapping (i.e. )
  Signs <- as.data.frame(lapply(Chains, function(mat, ind) {
    colMeans(mat[,ind]>0)
  }, ind=SignInd))
  
  VarSign <- apply(Signs, 1, var)
  
  if(SeveralLVs) {
    LV <- gsub(".*, ", "", colnames(Chains[[1]])[SignInd])
    #Chose variables who's sign will be used to swap other signs
    if(is.null(VarsToSwapBy)){
      VarsToSwapBy <- sapply(unique(LV), function(lv, vs) {
      vv <- vs[grep(lv, names(vs))]
      nm <- names(which(vv==max(vv)))
      if(length(nm)>1) nm <- nm[1] # probably something more sophisticated is better
      nm
    }, vs = VarSign, simplify = TRUE)
    }
    
  } else { # only 1 LV
    if(is.null(VarsToSwapBy)){
    #Chose variables who's sign will be used to swap other signs
    VarsToSwapBy <- names(which(VarSign==max(VarSign)))[1]
    }
  }
  
  }else if(rule==2){
    require(mousetrap)
    
    jointChains <- do.call(rbind, Chains)
    # bimodality score
    bms<-apply(jointChains[,SignInd],2,bimodality_coefficient)
    # find maximum bimodality score
    if(SeveralLVs){
    lstSgn <- sapply(unique(LV),function(lv)which.max(bms[grep(lv,names(bms))]),simplify=F)
    # formatting
    names(lstSgn) <- NULL
    VarsToSwapBy <- names(unlist(lstSgn))
    names(VarsToSwapBy) <- paste0(sort(unique(LV)))
    }else{
    lstSgn <- which.max(bms)
    # formatting
    VarsToSwapBy <- names(lstSgn)
    names(VarsToSwapBy) <- "1]"
    }
  }
  
  if(print) message(paste0("Swapping by ", paste(VarsToSwapBy, collapse=", ")))
  chains.sgn <- lapply(Chains, ReSignChain, vartosign=VarsToSwapBy, varinds=ProcessInds)

  as.mcmc.list(chains.sgn)
}
```

</details>

Finally, we can run the MCMC.
```{r MCMCrun, message=FALSE}
consts$nLVs = 1
chains.unsgn <- ParaNimble(4, fun = RunOneChain,
                       dat = dat,
                       code = OneLV.nim,
                       inits = inits, 
#                       Nburn = 5e1, NIter = 5.5e2, Nthin = 1, # for a small run
                       Nburn = 1e3, NIter = 2e3, Nthin = 1, # for a big HMC run
                       consts = consts, 
                       block = MakeBlockList(consts), slice = paste0("beta0[",1:consts$NSpecies,"]")) # HMC on the rest (hypers)
```
```{r MCMCrun_process, message=FALSE}
# post-process chains for sign-swapping
VarsToProcess <- c("^B", "^O", "^epsilon", "^varepsilon", "^z", "^gamma")
VarsToSign <- c("^B")
chains <- postProcess(chains.unsgn, VarsToProcess = VarsToProcess, 
                      VarToSign = VarsToSign, print = TRUE, rule = 2)#, VarsToSwapBy = "B[2]")
summ.HO = summary(chains)
```


### Results

There are a lot of parameters, so a lot of results to look at. Here we concentrate on $\boldsymbol{B}$, $\boldsymbol{\omega}$, $\boldsymbol{\sigma}$ and $\boldsymbol{\delta}$, and a latent variable-plot.

<details>
    <summary>We can look at the chains, to see that they have converged.</summary>

First the environmental effects:

```{r results1, warning=FALSE, message=FALSE}
library(basicMCMCplots)
chainsPlot(chains, var = c("B"), legend = F,traceplot = T)
```

Then the trait effects:

```{r ResO, warning=FALSE,message=FALSE, fig.height=10}
chainsPlot(chains, var = c("O"), legend = F,traceplot = TRUE)
```

</details>

These look quite OK (after post-processing the signs). Now we can create a plot of the site scores against their indices to inspect the results.

<details><summary>We use a plotting function, which is hidden here</summary>

```{r plotEnv1LVfunction, warning=FALSE,message=FALSE, fig.height=4}
PlotPost <- function(var, summ, varnames=NULL, ...) {
  vars <- grep(var, rownames(summ$statistics))
  if(is.null(varnames)) varnames <- rownames(summ$statistics)[grep(var, rownames(summ$statistics))]
  if(length(varnames)!=length(vars)) 
    stop(paste0("Number of variable names, ", length(varnames), "not the same as number of variables, ",
                length(vars)))
  
  plot(summ$statistics[vars,"Mean"], 1:length(vars), 
       xlim=range(summ$quantiles[vars,]), yaxt="n", ...)
  segments(summ$quantiles[vars,"2.5%"], 1:length(vars), summ$quantiles[vars,"97.5%"], 1:length(vars))
  segments(summ$quantiles[vars,"25%"], 1:length(vars), summ$quantiles[vars,"75%"], 1:length(vars), lwd=3)
  abline(v=0, lty=3)
  axis(2, at=1:length(vars), labels=varnames, las=1)
}
```
</details>

```{r plotEnv1LV, warning=FALSE,message=FALSE, fig.height=4}
par(mar = c(4.1, 4.5, 1, 1))
PlotPost(var = "B", summ = summ.HO, varnames = gsub("\\.", "\n", colnames(X)), 
         xlab = "Environmental Effect", ylab = "")

```

We can see that both Coarse Woody Debris (CWD) and canopy cover have positive effects on the latent variable, so ants that have a positive species score will be more abundant in areas with more canopy cover and CWD. 

We can also look at the trait effects. The bottom line is that there is too much uncertainty to say anything.


```{r plotTraits1LV, warning=FALSE,message=FALSE, fig.height=6}
par(mar=c(4.1, 7, 1, 1))
PlotPost(var = "O", summ = summ.HO, varnames = gsub("\\.", "\n", colnames(TR)), 
         xlab = "Trait Effect", ylab = "")

```

We can also calculate the full distributions of site and species scores, and plot them. We see that, well, there is variation, so the trait effects are not uncertain because the species scores are uncertain: it is because their effects are small.

```{r PlotScores, fig.height=14}
par(mfrow = c(2, 1), mar = c(2, 12, 4, 1))
PlotPost("^z", summ = summ.HO, varnames = NULL, ylab = "", xlab = "Latent variable 1", main = "Sites")
PlotPost("^gamma", summ = summ.HO, varnames = NULL, ylab = "", xlab = "Latent variable 1", main = "Species")

```

## More dimensions

More than one dimension requires adding extra identifiability constraints. Now that we have two or more latent variables, we also have to worry about their rotation.

<details><summary>For those who are interested, this is how we deal with the rotation</summary>
There are various ways to place the constraints, some more numerically stable than others. Generally, we note that the model on the link scale is similar to existing matrix decompositions, so that much about the required constraints can be learned from those.
 
To prevent rotational invariance, we require $\boldsymbol{B}$ to have zeros above the main diagonal. Note that for a number of latent variables greater than the number of predictors $K$, we need to add extra constraints to $\boldsymbol{\omega}$. van der Veen *et al* (2023) assumed $\boldsymbol{B}$ to be a semi-orthogonal matrix instead, as they encountered numerical instability with the constraints that we impose here. However, their constraint would require sampling on constrained parameter spaces, which is difficult, while this constraint formulation allows to use standard out-of-the-box MCMC samplers (also, it seems to work fine here). Note, that when $d \gt p,t $ the model is rotationally invariant, and additional constraints will need to be added.
</details>

<details><summary>The new model code is included here</summary>

```{r model2_code}
# Model code
# Update our constants from before with the new number of LVs, rest remains the same
# Model code
HO.nim <- nimbleCode({
  for (i in 1:NSites) {
    for (j in 1:NSpecies) {
      eta[i,j] <- beta0[j] + sum(gamma[j,1:nLVs]*sd.LV[1:nLVs]*z[i,1:nLVs])
      log(lambda[i, j]) <- eta[i, j]
      Y[i, j] ~ dpois(lambda[i, j])
    }      
    for (l in 1:nLVs) {
      XB[i, l] <- sum(X[i, 1:NEnv]*BSTAR[1:NEnv, l])
      epsilonSTAR[i,l] ~ dnorm(0,sd.Site[l])#Residual
      zSTAR[i,l] <- XB[i,l] + epsilonSTAR[i,l]
    }
  }
  
  for(j in 1:NSpecies) {
    for (l in 1:nLVs) {
      omegaTR[j, l] <- sum(TR[j, 1:NTraits]*OSTAR[1:NTraits, l])
      varepsilonSTAR[j,l] ~ dnorm(0,sd.Species[l]) # Residual
      gammaSTAR[j,l] <- omegaTR[j,l] + varepsilonSTAR[j,l]
    }
    
    beta0[j] ~ dnorm(0, sd=1)
  }
  # Constraints to 0 on upper diagonal
  # stole some code from Boral for this - thanks Francis
  for(l in 1:(nLVs-1)) { 
    for(ll in (l+1):(nLVs)) {
      BSTAR[l,ll] <- 0 
      OSTAR[l,ll] <- 0 
    } 
  }

    
  for(l in 1:nLVs) { 
    # diagonal elements
    BSTAR[l,l] ~ dnorm(0,sd=1)
    OSTAR[l,l] ~ dnorm(0,sd=1)

    ## standardizing z and gamma
    StdDev.z[l] <- sd(zSTAR[1:NSites,l])
    z[1:NSites,l] <- zSTAR[1:NSites,l]/StdDev.z[l]
    
    StdDev.gamma[l] <- sd(gammaSTAR[1:NSpecies,l])
    gamma[1:NSpecies,l] <- gammaSTAR[1:NSpecies,l]/StdDev.gamma[l]
    
  # Scale other parameters
    epsilon[1:NSites,l] <- epsilonSTAR[1:NSites,l]/StdDev.z[l]
    B[1:NEnv,l] <-  BSTAR[1:NEnv,l]/StdDev.z[l]
    varepsilon[1:NSpecies,l] <- varepsilonSTAR[1:NSpecies,l]/StdDev.gamma[l]
    O[1:NTraits,l] <-  OSTAR[1:NTraits,l]/StdDev.gamma[l]
  
    # priors for scales
    sd.Site[l] ~ dexp(1)
    sd.Species[l] ~ dexp(1)
    sd.LV[l] ~ dexp(1)
    } 
  
  ## Free lower diagonals
  for(l in 2:nLVs) { 
    for(ll in 1:(l-1)) { 
      OSTAR[l,ll] ~ dnorm(0,sd=1)
      BSTAR[l,ll] ~ dnorm(0,sd=1)
      } 
    }
  for(l in (nLVs+1):NEnv) { 
    for(ll in 1:(nLVs)) { 
      BSTAR[l,ll] ~dnorm(0,1) 
    } 
  } ## All other elements
  for(l in (nLVs+1):NTraits) { 
    for(ll in 1:(nLVs)) { 
      OSTAR[l,ll] ~dnorm(0,1) 
    } 
  } ## All other elements
})
```

</details>

<details><summary>We create a new function to simulate starting values from the prior distributions, which we can hide away</summary>

```{r init2}
inits <- function(consts){
    B = matrix(rnorm(consts$nLVs*consts$NEnv),ncol=consts$nLVs)
    B[upper.tri(B)] = 0
    O = matrix(rnorm(consts$nLVs*consts$NTraits),nrow=consts$NTraits)
    O[upper.tri(O)] = 0
    varepsilon = mvtnorm::rmvnorm(consts$NSpecies,rep(0,consts$nLVs),diag(consts$nLVs))
    epsilon = mvtnorm::rmvnorm(consts$NSites,rep(0,consts$nLVs),diag(consts$nLVs))
    list(
        BSTAR = B,
        OSTAR = O,
        epsilonSTAR = epsilon,
        varepsilonSTAR = varepsilon,
        sd.Site = rexp(consts$nLVs),
        sd.Species = rexp(consts$nLVs),
        beta0 = rnorm(consts$NSpecies),
        sd.LV = rexp(consts$nLVs)
    )
}
```
</details>

<details><summary>We rotate the ordination after the MCMC to make it more interpretable. Here is code to do that, and an explanation of what is done.</summary>

Our model is:

$$
\eta_{ij} = \beta_{0j} + \boldsymbol{z}_i^\top \boldsymbol{\Sigma} \boldsymbol{\gamma}_j.
$$

But we can rotate by a rotation matrix $M$ this to get an equivalent model:

$$
\eta_{ij} = \beta_{0j} + \boldsymbol{z}_j^\top \boldsymbol{M} \boldsymbol{\Sigma} \boldsymbol{M}^\top\boldsymbol{\gamma}_i.
$$
There are many possible rotations (see the [https://cran.r-project.org/web/packages/GPArotation/index.html](GPArotation) package for a long list). Here we will just rotate abased on a singular value decomposition (SVD) of the latent variables, as in the *gllvm* *R*-package [Niku *et al.*](https://cran.r-project.org/web/packages/gllvm/index.html). This rotates the ordination so that most of the variation in $\boldsymbol{z}_i^\top \boldsymbol{\Sigma}$ is put on the first latent variable, and so that the second latent variable has the next largest amount of variance. Note, that this does not guarantee that these are the latent variables that explain most variation in the response, unlike in eigenanalysis.

In detail, for each draw from the posterior we:

1. Multiply the LV-specific standard deviations by the column-wise standard deviation of $\boldsymbol{Gamma}$ and/or $\boldsymbol{Z}$ as deemed appropriate
2. Column-wise re-scale $\boldsymbol{B}, \boldsymbol{E}, \boldsymbol{\Gamma}, \boldsymbol{Z}, \boldsymbol{\Omega}$ and $\boldsymbol{\mathcal{E}}$: so that both $\boldsymbol{Z}$ and $\boldsymbol{\Gamma}$ have column-wise unit variance.
3. Calculate a rotation matrix $\boldsymbol{M}$ from $\boldsymbol{Z}\Sigma$
4. Rotate $\boldsymbol{B}, \boldsymbol{E}, \Gamma, \boldsymbol{Z}, \boldsymbol{\Omega}$ and $\boldsymbol{\mathcal{E}}$
5. Column-wise (LV-wise) re-calculate the scale of $\boldsymbol{Z}$, $\boldsymbol{E}$, $\boldsymbol{\mathcal{E}}$

The first step sweeps the scale of the LVs and loadings into the LV-specific scale so it still represents the total variance of the latent variables.

```{r RotateScores}

# Function to convert a variable in an MCMC chain that should be a matrix from 
# a vector to the right matrix
ChainToMatrix <- function(ch, name) {
  v <- ch[grep(paste0("^", name, "\\["), names(ch))]
  l.row <- as.numeric(gsub(".*\\[", "", gsub(",.*", "", names(v))))
  l.col <- as.numeric(gsub(".*, ", "", gsub("\\]", "", names(v))))
  mat <- matrix(0, nrow=max(l.row), ncol=max(l.col))
  mat[l.row+max(l.row)*(l.col-1)]<-v
  mat
}

# Function to convert a variable in an MCMC chain that should be a vector to a diagonal matrix
ChainToDiag <- function(ch, name) {
  v <- ch[grep(paste0("^", name, "\\["), names(ch))]
  mat <- diag(v)
  mat
}

# Function to rotate a latent variable, defaults to Varimax. Methods in GPArotation package
# ch: one draw from MCMC chain
# RotateBy: name of variable to rotate by (i.e. to calculate the rotation matrix for)
# SDby: name of Standard deviations
# ToRotate: Variables to rotate (e.g. epsilons, gamma/z).
# SDToRotate: Standatd devations to rotate
# rotation.fn: Function in GPArotation to calcualte rotation, or svd. Defaults to svd.
# scale: Should RotateBy be scaled by SDby before rotating? Defaults to FALSE
rotate2LV <- function(ch, RotateBy, SDby, ToRotate=NULL, SDToRotate=NULL, rotation.fn="svd", 
                      scale=FALSE) {
  require(GPArotation)
  # Function to rotate matrices & return full vector or a matrix (retmat)
  RotateVar <- function(name, vec, rotmat, retmat=FALSE) {
    mat <- ChainToMatrix(ch=vec, name=name)
    rotated <- mat%*%rotmat
    
    if(retmat) {
      res <- rotated
    } else {
      vec[grep(paste0("^", name, "\\["), names(vec))] <- c(rotated)
      res <- vec
    }
    res
  }
    # Function to rotate (diagonal) matrices & return the diagonal
  RotateSD <- function(name, vec, rotmat) {
    mat <- diag(vec[grep(paste0("^", name, "\\["), names(vec))])
    rotated <- mat%*%rotmat
    
    vec[grep(paste0("^", name, "\\["), names(vec))] <- diag(rotated)
    vec
  }

  # Extract SDs and latent variables
  SDs <- ch[grep(SDby, names(ch))]
  LVmat <- ChainToMatrix(ch, RotateBy)
  if(scale) LVmat <- sweep(LVmat, 2, SDs, "*")
  
  # Calculate a rotation matrix
  if(rotation.fn!="svd"){
  loadings.r  <- do.call(rotation.fn, list(A=LVmat)) # rotate
  }else{
  rot <- svd(LVmat)$v
  loadings.r <- list(Th=rot, loadings = LVmat%*%rot)
  }
  
  # Retrieve correct scale of LVs
  if(scale){
    #also need to put LVs into here
    ch[grep(paste0("^", RotateBy, "\\["), names(ch))]  <- scale(loadings.r$loadings, center = F)
    ch[grep(paste0("^", SDby, "\\["), names(ch))] <- attr(scale(loadings.r$loadings, center = F),"scaled:scale")
  } else{
   # if LVs are not scaled by LV sds, we just add the new LV scale to LVs SDs.
     ch[grep(paste0("^", RotateBy, "\\["), names(ch))] <- scale(loadings.r$loadings, center = F)
     ch[grep(paste0("^", SDby, "\\["), names(ch))] <- attr(scale(loadings.r$loadings, center = F),"scaled:scale")*SDs
  }

  # Rotate rest of the variables
  for(nm in unique(ToRotate)) {
    ch <- RotateVar(vec=ch, name=nm, rotmat=loadings.r$Th, retmat = FALSE)
  }
   for(nm in unique(SDToRotate)) {
    ch <- RotateSD(vec=ch, name=nm, rotmat=loadings.r$Th)
  }

  ch
}

RotateChains <- function(mcmc.lst, RotateBy="z", SDby = "sd.LV", 
                         ToRotate=c("B", "epsilon", "gamma", "O", "varepsilon"), 
                         SDToRotate=c("sd.Site", "sd.Species"), ...) {
  if(SDby%in%SDToRotate)stop("'SDby' should not be present in 'SDToRotate'.")
  if(RotateBy%in%ToRotate)stop("'RotateBy' should not be present in 'ToRotate'.")
  rotated <- lapply(mcmc.lst, function(mcmc) {
    rot <- apply(mcmc, 1, rotate2LV, RotateBy=RotateBy, SDby = SDby, 
                         ToRotate=ToRotate, SDToRotate=SDToRotate, ...)
    as.mcmc(t(rot))
  })
  as.mcmc.list(rotated)
}

RescaleVars <- function(vec, ScaleBy, SDTorescale,ToRescale) {
  if(!any(c("z","gamma")%in%ScaleBy))stop("Not a valid choice.")
  if("sd.LV"%in%SDTorescale)stop("Not a valid choice.")
  vec2 <- vec
  # get scale from z or gamma
  ScBy <- ChainToMatrix(vec, ScaleBy)
  SD <- apply(ScBy, 2, sd)
  
  for(nm in unique(c(ScaleBy, ToRescale))) {
      Sc.x <- ChainToMatrix(vec, nm)
      Sc.x.sc <- sweep(Sc.x, 2, SD, "/")
      vec2[grep(paste0("^", nm, "\\["), names(vec2))] <- c(Sc.x.sc)
  }

  # scale sd separately
  for(nm in SDTorescale) {
    SD.x <- vec[grep(paste0("^", nm, "\\["), names(vec))]
    vec2[grep(paste0("^", nm, "\\["), names(vec2))] <- SD.x/SD
  }
  
  # scale into sd.LV
  # sd.LV <- vec[grep("sd.LV", names(vec))]
  # vec2[grep("sd.LV", names(vec2))] <- SD*sd.LV
  
  vec2
}
RescaleChains <- function(mcmc.lst, ...) {
  rescale <- lapply(mcmc.lst, function(mcmc) {
    rot <- apply(mcmc, 1, RescaleVars, ...)
    as.mcmc(t(rot))
  })
  as.mcmc.list(rescale)
}
```

</details>

We can run the fitting with the same code as before:

```{r mcmc2_run}
consts$nLVs <- nLVs <- 2
HO.2LV <- ParaNimble(4, fun = RunOneChain,
                       dat = dat,
                       code = HO.nim,
                       inits = inits, block = MakeBlockList(consts),
#                       Nburn=5e1, NIter=5.5e2, Nthin=1, # for a small run
                       Nburn = 1000, NIter = 4e3, Nthin = 1, # for a full run
                       consts = consts)
```

```{r mcmc2_process}
#don't need to rescale, have already been rescaled mid-run
# # Re-scale gamma
# chains2LV.sc <- RescaleChains(HO.2LV, ScaleBy = "gamma", 
#                               SDTorescale = c("sd.Species"),
#                               ToRescale = c("gamma",  "O", "varepsilon"))
# 
# # Re-scale Z
# chains2LV.sc2 <- RescaleChains(chains2LV.sc, ScaleBy = "z", 
#                               SDTorescale = c("sd.Site"), 
#                               ToRescale = c("z",  "B", "epsilon"))

# Now rotate based on Z*Sigma and return its scale as LVsd.
chains2LV.r <- RotateChains(HO.2LV, rotation.fn = "svd", scale = T)

# post-process chains for sign-swapping
chains2LV <- postProcess(chains2LV.r, VarsToProcess = VarsToProcess, rule = 2, print=T)

# Calculate summaries
summ.HO2LV <- summary(chains2LV)

rm(chains2LV.sc, chains2LV.sc2, chains2LV.r)

```


## Results

<details><summary>First, we look at the mixing.</summary>
Environment first:

```{r results2, fig.height=10}
chainsPlot(chains2LV, var = c("B"), legend = F, traceplot=TRUE)
```

And the trait effects:

```{r ResO2LV, warning=FALSE,message=FALSE, fig.height=25}
chainsPlot(chains2LV, var = c("O"), legend = F, traceplot=TRUE)

```

</details>
What we are really interested in, is looking at the environment-trait interactions. We can plot those with the following code.

```{r EnvTraitInt}
# Function to get MCMC results for reduced-rank approximated interaction term
getMCMCInt <- function(mcmc.lst, dat,...) {
  int.mcmc <- lapply(mcmc.lst, function(mcmc){
    inCoefs <- as.mcmc(t(apply(mcmc, 1, function(ch){
    B <- ChainToMatrix(ch,"B")
    O <- ChainToMatrix(ch,"O")  
    SDs <- diag(ch[grep("sd.LV",gsub("\\s*\\[[^\\)]+\\]","",names(ch)))])
    coefs <- B%*%SDs%*%t(O)
    vec <- c(coefs)
    names(vec) <- paste0(colnames(dat$X),":",rep(colnames(dat$TR),each=ncol(dat$X)))
    vec
    })))
  }
  )
  as.mcmc.list(int.mcmc)
}

intChains <- getMCMCInt(chains2LV, dat = dat)
summ.EnvTraitInt <- summary(intChains)
coefs <- matrix(summ.EnvTraitInt$statistics[,1], ncol = ncol(dat$TR), nrow=ncol(dat$X), dimnames = list(colnames(dat$X),colnames(dat$TR)))
a <- 0.2
colort <- colorRampPalette(c("blue", "white", "red"))
lattice::levelplot(coefs, xlab = "Environmental Variables", 
                      ylab = "Species traits", col.regions = colort(100), cex.lab = 1.3, 
                      at = seq(-a, a, length = 100), scales = list(x = list(rot = 45)))
```

Now, we can make two-dimensional ordination plots of sites and species, with their predictor effects. Note that these have been rotated, so that most ofthe variance should be on the first axis.

<details><summary>We need a function to plot the arrows, which is hidden here</summary>
```{r AddArrows}
AddArrows <- function(coords, marg= par("usr"), col=2) {
  origin <- c(mean(marg[1:2]), mean(marg[3:4]))
  Xlength <- sum(abs(marg[1:2]))/2
  Ylength <- sum(abs(marg[3:4]))/2
  ends <- coords / max(abs(coords)) * min(Xlength, Ylength) * .8
  arrows(
    x0 = origin[1],
    y0 = origin[2],
    x1 = ends[,
              1] + origin[1],
    y1 = ends[, 2] + origin[2],
    col = col,
    length = 0.1)

  text(
    x = origin[1] + ends[, 1] * 1.1,
    y = origin[2] + ends[, 2] * 1.1,
    labels = rownames(coords),
    col = col)
  
}

```
</details>

```{r results2_site, warning=FALSE, fig.height=9}

ExtractMeans <- function(mcmc.lst, name=NULL) { # this needs defensive programming
  if(is.null(name)) {
    ind <- 1:nrow(mcmc.lst$statistics)
  } else {
    ind <- grep(name, rownames(mcmc.lst$statistics))
  }
  Means <- mcmc.lst$statistics[ind,"Mean"]
  if(any(grep(",", names(Means)))) {
    Col <- gsub("]", "", unique(gsub(".*, ", "", names(Means))))
    res <- sapply(Col, function(cc, mn) {
      mn[grep(paste0(cc,"]"), names(mn))]
    }, Means)
  } else {
    res <- Means
  }
  res
}
GetMeans <- function(summ, name, d=1) {
  if(is.null(d)) d <- 1
  var <- summ$statistics[grep(paste0('^', name), rownames(summ$statistics)),"Mean"]
  matrix(var,ncol=d)
}


# LVMeans <- ExtractMeans(LVs, name = "^LV\\[")
# gammaMeans <- ExtractMeans(gamma, name="^gamma")

z.m <- GetMeans(summ.HO2LV, name="^z", d=consts$nLVs)
gamma.m <- GetMeans(summ.HO2LV, name="^gamma", d=consts$nLVs)
vareps.m <- GetMeans(summ.HO2LV, name="^varepsilon", d=consts$nLVs)
eps.m <- GetMeans(summ.HO2LV, name="^epsilon", d=consts$nLVs)
B.m <- GetMeans(summ.HO2LV, name="B", d=consts$nLVs)
row.names(B.m) <- colnames(dat$X)
O.m <- GetMeans(summ.HO2LV, name="O", d=consts$nLVs)
row.names(O.m) <- colnames(dat$TR)

par(mfrow=c(2,1))
#LVs
plot(z.m,type="n", xlab="Latent variable 1", ylab="Latent variable 2", main = "Sites")
text(z.m,labels=1:consts$NSites)
AddArrows(marg = par("usr"), coords=B.m, col="red")

#gammas
plot(gamma.m,type="n", xlab="Latent variable 1", ylab="Latent variable 2", main = "Species")
text(gamma.m,labels=vegan::make.cepnames(colnames(Y)))
AddArrows(marg = par("usr"), coords=O.m, col="blue")
```

We can see that almost all of the environmental effects seem to act in the same way, mainly on LV1.


```{r sum_stats}
Ratio <- chains2LV$chain1[,"sd.LV[1]"]/chains2LV$chain1[,"sd.LV[2]"]
Ratio.u <- HO.2LV$chain1[,"sd.LV[1]"]/HO.2LV$chain1[,"sd.LV[2]"]

SDs <- summ.HO2LV$statistics[grep("sd", rownames(summ.HO2LV$statistic)),]
rownames(SDs) <- c(paste0("Latent Variable ", 1:2),
              paste0("Site ", 1:2),
              paste0("Species ", 1:2))
knitr::kable(SDs, digits=2, format = "html", table.attr = "style='width:70%;'")
```


\Large **INSERT VAREXPL INFO**

These tell us how well the predictors explain the ordination; the species- and site-specific scale parameters would be zero if the predictors fully explained the ordination. The scale parameters for the latent variables are similar to singular values (the square root of eigenvalues) in a classical ordination; they reflect a dimension its importance to the response.

## Residual covariance matrix

The covariance of species is determined by traits, LV-specific variation and sites' residual variation, and the covariance between sites is determined by the environment, LV-specific variation and species' residual variation.

<details><summary>The maths behind this is covered in here</summary>
The residual covariance matrix of the model (where you calculate species associations from) is determined by three terms:

1) $\boldsymbol{X}\boldsymbol{B}\boldsymbol{\Sigma}\boldsymbol{\varepsilon}_j \sim \mathcal{N}(0,\boldsymbol{X}\boldsymbol{B}\boldsymbol{\Sigma}\text{diag}(\boldsymbol{\delta}^2)\boldsymbol{\Sigma}\boldsymbol{B}^\top\boldsymbol{X}^\top)$
2) $\boldsymbol{TR}\boldsymbol{\omega}\boldsymbol{\Sigma}\boldsymbol{\epsilon}_i\sim \mathcal{N}(0,\boldsymbol{T}\boldsymbol{\omega}\boldsymbol{\Sigma}\text{diag}(\boldsymbol{\sigma}^2)\boldsymbol{\Sigma}\boldsymbol{\omega}^\top\boldsymbol{T}^\top)$
3) $\boldsymbol{\epsilon}_i^\top\boldsymbol{\varepsilon}_j \sum \limits^{d}_{q=1}\Sigma_{q,q}\sigma_q\delta_q\sim \mathcal{K}_0(\sigma^{-1}_q\delta^{-1}_q\vert\epsilon_{iq}\varepsilon_{iq}\vert)$,

where $\mathcal{K}_0$ denotes the zero order modified Bessel function of the second kind. The first term tells us that correlations between species are determined by the environment. The second term tells us that the correlation between sites is determined by species traits. The last term induces no correlation between sites and species for diagonal $\boldsymbol{\Sigma}$, but serves to scale species associations. Consequently, the covariance for species $j,j2$ and site $i,i2$ is:
\begin{multline}
\Sigma_{i,i2,j,j2} =
\text{cov}(\boldsymbol{x}_i^\top\boldsymbol{B}\boldsymbol{\Sigma}\boldsymbol{\varepsilon}_j,\boldsymbol{x}_{i2}^\top\boldsymbol{B}\boldsymbol{\Sigma}\boldsymbol{\varepsilon}_{j2}) +
\text{cov}(\boldsymbol{x}_i^\top\boldsymbol{B}\boldsymbol{\Sigma}\boldsymbol{\varepsilon}_j,\boldsymbol{tr}_{j2}^\top\boldsymbol{\omega}\boldsymbol{\Sigma}\boldsymbol{\epsilon}_{i2}) +
\text{cov}(\boldsymbol{x}_i^\top\boldsymbol{B}\boldsymbol{\Sigma}\boldsymbol{\varepsilon}_j,\boldsymbol{\epsilon}_{i2}^\top\boldsymbol{\Sigma}\boldsymbol{\varepsilon}_{j2}) + 
\text{cov}(\boldsymbol{tr}_j^\top\boldsymbol{\omega}\boldsymbol{\Sigma}\boldsymbol{\epsilon}_i,\boldsymbol{x}_{i2}^\top\boldsymbol{B}\boldsymbol{\Sigma}\boldsymbol{\varepsilon}_{j2}) +
\text{cov}(\boldsymbol{tr}_{j}^\top\boldsymbol{\omega}\boldsymbol{\Sigma}\boldsymbol{\epsilon}_{i},\boldsymbol{tr}_{j2}^\top\boldsymbol{\omega}\boldsymbol{\Sigma}\boldsymbol{\epsilon}_{i2}) + \\
\text{cov}(\boldsymbol{tr}_j^\top\boldsymbol{\omega}\boldsymbol{\Sigma}\boldsymbol{\epsilon}_i,\boldsymbol{\epsilon}_{i2}^\top \boldsymbol{\Sigma}\boldsymbol{\varepsilon}_{j2}) +
\text{cov}(\boldsymbol{\epsilon}_{i}^\top \boldsymbol{\Sigma}\boldsymbol{\varepsilon}_{j},\boldsymbol{x}_{i2}^\top\boldsymbol{B}\boldsymbol{\Sigma}\boldsymbol{\varepsilon}_{j2}) +
\text{cov}(\boldsymbol{\epsilon}_{i}^\top \boldsymbol{\Sigma} \boldsymbol{\varepsilon}_{j},\boldsymbol{tr}_{j2}^\top\boldsymbol{\omega}\boldsymbol{\Sigma}\boldsymbol{\epsilon}_{i2})+
\text{cov}(\boldsymbol{\epsilon}_i^\top \boldsymbol{\Sigma}\boldsymbol{\varepsilon}_j,\boldsymbol{\epsilon}_{i2}^\top \boldsymbol{\Sigma}\boldsymbol{\varepsilon}_{j2}),
\end{multline}
third order terms are zero for central normal random variables, so this simplifies to:
\begin{equation}
\Sigma_{i,i2,j,j2} =
\boldsymbol{x}_i^\top\boldsymbol{B}\boldsymbol{\Sigma}\text{cov}(\boldsymbol{\varepsilon}_j,\boldsymbol{\varepsilon}_{j2}) \boldsymbol{\Sigma}\boldsymbol{B}^\top\boldsymbol{x}_{i2}+
\boldsymbol{x}_i^\top\boldsymbol{B}\boldsymbol{\Sigma}\text{cov}(\boldsymbol{\varepsilon}_j,\boldsymbol{\epsilon}_{i2})\boldsymbol{\Sigma}\boldsymbol{\omega}^\top\boldsymbol{tr}_{j2} +
\boldsymbol{tr}_j^\top\boldsymbol{\omega}\boldsymbol{\Sigma}\text{cov}(\boldsymbol{\epsilon}_i,\boldsymbol{\varepsilon}_{j2})\boldsymbol{\Sigma}\boldsymbol{B}^\top\boldsymbol{x}_{i2} +
\boldsymbol{tr}_{j}^\top\boldsymbol{\omega}\boldsymbol{\Sigma}\text{cov}(\boldsymbol{\epsilon}_{i},\boldsymbol{\epsilon}_{i2})\boldsymbol{\Sigma}\boldsymbol{\omega}^\top\boldsymbol{tr}_{j2} +
\text{cov}(\boldsymbol{\epsilon}_i^\top \boldsymbol{\Sigma}\boldsymbol{\varepsilon}_j,\boldsymbol{\epsilon}_{i2}^\top \boldsymbol{\Sigma} \boldsymbol{\varepsilon}_{j2}).
\end{equation}
Here, $\text{cov}(\boldsymbol{\varepsilon}_j,\boldsymbol{\epsilon}_{i2}) = \text{cov}(\boldsymbol{\epsilon}_i,\boldsymbol{\varepsilon}_{j2}) = 0$, and for  $\text{cov}(\boldsymbol{\epsilon}_i^\top\boldsymbol{\Sigma}\boldsymbol{\varepsilon}_j,\boldsymbol{\epsilon}_{i2}^\top\boldsymbol{\Sigma}\boldsymbol{\varepsilon}_{j2}) = 2\text{tr}(\text{diag}(\boldsymbol{\delta}^2)\boldsymbol{\Sigma}\text{diag}(\boldsymbol{\sigma}^2)\boldsymbol{\Sigma})$. Further, $\text{cov}(\boldsymbol{\varepsilon}_j,\boldsymbol{\varepsilon}_{j2})$ is zero for $j \neq j2$ and $\text{diag}(\boldsymbol{\delta}^2)$ otherwise, and similar for $\text{cov}(\boldsymbol{\epsilon}_i,\boldsymbol{\epsilon}_{i2})$ .

Consequently, for a species association matrix where we consider the block of the covariance matrix where $i = i2$, or for the site-to-site matrix where we consider the block $j  = j2$, we have:

\begin{split}
\Sigma_{j,j2} &=
\boldsymbol{x}_i^\top\boldsymbol{B}\boldsymbol{\Sigma}\text{cov}(\boldsymbol{\varepsilon}_j,\boldsymbol{\varepsilon}_{j2}) \boldsymbol{\Sigma}\boldsymbol{B}^\top\boldsymbol{x}_{i} +
\text{cov}(\boldsymbol{\epsilon}_i^\top \boldsymbol{\Sigma}\boldsymbol{\varepsilon}_j,\boldsymbol{\epsilon}_{i}^\top \boldsymbol{\Sigma} \boldsymbol{\varepsilon}_{j2}) + 
\boldsymbol{tr}_{j}^\top\boldsymbol{\omega}\boldsymbol{\Sigma}\text{var}(\boldsymbol{\epsilon}_{i},\boldsymbol{\epsilon}_{i})\boldsymbol{\Sigma}\boldsymbol{\omega}^\top\boldsymbol{tr}_{j2} \\
&= \boldsymbol{x}_i^\top\boldsymbol{B}\boldsymbol{\Sigma}\text{cov}(\boldsymbol{\varepsilon}_j,\boldsymbol{\varepsilon}_{j2}) \boldsymbol{\Sigma}\boldsymbol{B}^\top\boldsymbol{x}_{i} +
\boldsymbol{tr}_{j}^\top\boldsymbol{\omega}\boldsymbol{\Sigma}\text{diag}(\boldsymbol{\sigma}^2)\boldsymbol{\Sigma}\boldsymbol{\omega}^\top\boldsymbol{tr}_{j2}+
2\text{tr}\{\text{diag}(\boldsymbol{\delta}^2)\boldsymbol{\Sigma}\text{diag}(\boldsymbol{\sigma}^2)\boldsymbol{\Sigma}\},
\end{split}

and 

\begin{equation}
\Sigma_{i,i2} = 
\boldsymbol{tr}_{j}^\top\boldsymbol{\omega}\boldsymbol{\Sigma}\text{cov}(\boldsymbol{\epsilon}_{i},\boldsymbol{\epsilon}_{i2})\boldsymbol{\Sigma}\boldsymbol{\omega}^\top\boldsymbol{tr}_{j} +
\boldsymbol{x}_i^\top\boldsymbol{B}\boldsymbol{\Sigma}\text{diag}(\boldsymbol{\delta}^2)\boldsymbol{\Sigma}\boldsymbol{B}^\top\boldsymbol{x}_{i2} + 
2\text{tr}\{\text{diag}(\boldsymbol{\delta}^2)\boldsymbol{\Sigma}\text{diag}(\boldsymbol{\sigma}^2)\boldsymbol{\Sigma}\}.
\end{equation}
</details>

 We can visualize those matrices here for (e.g.,) the first site and first species, respectively.

```{r residualcov}
#species
spec.mat <- matrix(0,nrow=consts$NSpecies,ncol=consts$NSpecies)
Sigma <- diag(SDs[grep("Latent Variable", rownames(SDs)),1]^2)
Sigma.sp <- diag(SDs[grep("Species", rownames(SDs)),1]^2)
Sigma.si <- diag(SDs[grep("Site", rownames(SDs)),1]^2)

for(j in 1:consts$NSpecies){
  for(j2 in 1:consts$NSpecies){
    if(j==j2){
      spec.mat[j,j2] = X[1,,drop=F]%*%B.m%*%Sigma%*%Sigma.sp%*%t(B.m)%*%t(X[1,,drop=F])
    }
    spec.mat[j,j2] = spec.mat[j,j2] + TR[j,,drop=F]%*%O.m%*%Sigma%*%Sigma.si%*%Sigma%*%t(O.m)%*%t(TR[j2,,drop=F]) + 2*sum(diag((Sigma.sp%*%Sigma%*%Sigma.si%*%Sigma)))
  }
}

spec.cor.mat <- cov2cor(spec.mat)
colnames(spec.cor.mat) <- row.names(spec.cor.mat) <- colnames(Y)
corrplot::corrplot(spec.cor.mat,type = "lower",order = "AOE", main = "Species", mar = c(1,1,1,1),tl.srt=45,tl.cex = .5)

#sites
site.mat <- matrix(0,nrow=consts$NSites,ncol=consts$NSites)

for(i in 1:consts$NSites){
  for(i2 in 1:consts$NSites){
    if(i==i2){
      site.mat[i,i2] = TR[1,,drop=F]%*%O.m%*%Sigma%*%Sigma.si%*%t(O.m)%*%t(TR[1,,drop=F])
    }
    site.mat[i,i2] = site.mat[i,i2] + X[i,,drop=F]%*%B.m%*%Sigma%*%Sigma.sp%*%Sigma%*%t(B.m)%*%t(X[i2,,drop=F]) + 2*sum(diag((Sigma.sp%*%Sigma%*%Sigma.si%*%Sigma)))
  }
}

site.cor.mat <- cov2cor(site.mat)
colnames(site.cor.mat) <- row.names(site.cor.mat) <- paste("Site", 1:consts$NSites)
corrplot::corrplot(site.cor.mat,type = "lower",order = "AOE", main = "Sites", mar = c(3,3,3,3),tl.srt=45,tl.cex = .5)
```
